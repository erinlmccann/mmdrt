{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580a16cc-bef3-4022-9635-09778fd4d5aa",
   "metadata": {},
   "source": [
    "## Telemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a658ea0-9e31-45e3-a0d5-28506df3bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import re\n",
    "from typing import Union, List, Tuple\n",
    "\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e4cf9-bdda-492b-a706-cab1d4da123f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_parquet(file_path: str,\n",
    "                   output_directory: str):\n",
    "    \"\"\"Convert CSV files to Parquet file format.\"\"\"\n",
    "\n",
    "    # create output file name\n",
    "    output_parquet_file = os.path.join(output_directory, f\"{os.path.splitext(os.path.basename(file_path))[0]}.parquet\")\n",
    "    \n",
    "    # construct query to convert to parquet files\n",
    "    sql = f\"\"\"\n",
    "    COPY(\n",
    "        SELECT\n",
    "            *\n",
    "        FROM \n",
    "            '{file_path}'\n",
    "        ) TO '{output_parquet_file}' (FORMAT PARQUET);\n",
    "    \"\"\"\n",
    "    \n",
    "    # execute conversion\n",
    "    duckdb.query(sql)\n",
    "    \n",
    "    return output_parquet_file\n",
    "\n",
    "\n",
    "def read_tagging_file(tagging_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Read in raw excel tagging file to data frame and add field for PST date time fields.\"\"\"\n",
    "    \n",
    "    # read in tagging data\n",
    "    df = pd.read_excel(tagging_file)\n",
    "\n",
    "    # rename fields\n",
    "    df.rename(columns={\"rel_datetime\": \"tag_release_date\"}, inplace=True)\n",
    "\n",
    "    # adjust date times in tagging file to PST\n",
    "    df[\"tag_activation_date_pst\"] = pd.to_datetime(df[\"tag_activation_date\"]) - pd.Timedelta(hours=1)\n",
    "    df[\"tag_release_date_pst\"] = pd.to_datetime(df[\"tag_release_date\"]) - pd.Timedelta(hours=1)\n",
    "    \n",
    "    return df.sort_values(by=\"fish_id\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "def read_beacon_file(beacon_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Read in raw beacon file to data frame.\"\"\"\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "def generate_directory_list(target_directory: str,\n",
    "                            ignore_dirs: Tuple[str] = (\".DS_Store\", \"pre_study\", \"zips\", \".zip\")) -> list:\n",
    "    \"\"\"Generate a clearn directory list.\"\"\"\n",
    "    \n",
    "    return [os.path.join(target_directory, i) for i in os.listdir(target_directory) if i not in ignore_dirs]\n",
    "\n",
    "\n",
    "def add_trailing_zeros(x: str, \n",
    "                       length: int):\n",
    "    \"\"\"Add trailing zeros to a string.\"\"\"\n",
    "    \n",
    "    return(x + f\"{'0' * (length - len(x))}\")\n",
    "\n",
    "\n",
    "def add_leading_zeros(x: str, \n",
    "                    length: int):\n",
    "    \"\"\"Add leading zeros to a string.\"\"\"\n",
    "    \n",
    "    return(f\"{'0' * (length - len(x))}\" + x)\n",
    "\n",
    "\n",
    "def validate_file_to_directory_match(file_list: list):\n",
    "    \"\"\"Validate file names to their parent directory to ensure that no errors have occurred.\"\"\"\n",
    "\n",
    "    valid_list = []\n",
    "    \n",
    "    for i in file_list:\n",
    "\n",
    "        # get the source directory name of the download\n",
    "        source_directory = os.path.basename(os.path.dirname(i))\n",
    "\n",
    "        # extact the file parts to match to source directory\n",
    "        target_file_parts = os.path.splitext(os.path.basename(i))\n",
    "        target_file_base = target_file_parts[0].split(\"_\")[-1]\n",
    "        target_file_extension = target_file_parts[1]\n",
    "\n",
    "        if source_directory != target_file_base:\n",
    "            print(f\"File '{i}' does not match the parent directory name.\")\n",
    "            print(\"Removing from inputs.  Please review.\")\n",
    "\n",
    "        else:\n",
    "            valid_list.append(i)\n",
    "\n",
    "    return valid_list\n",
    "\n",
    "\n",
    "def generate_orion_import_file_list(orion_dir: str):\n",
    "    \"\"\"Generate a list of orion files to import after validation.\"\"\"\n",
    "\n",
    "    # generate full path lists of text and hex files in the orion directory\n",
    "    text_files = glob.glob(os.path.join(orion_dir, \"**/*.txt\"))\n",
    "    hex_files = glob.glob(os.path.join(orion_dir, \"**/*.hex\"))\n",
    "\n",
    "    # validate file name to directory name match\n",
    "    text_files = validate_file_to_directory_match(text_files)\n",
    "    hex_files = validate_file_to_directory_match(hex_files)\n",
    "\n",
    "    # validate to ensure a hex / text pair\n",
    "    text_file_base_list = [os.path.splitext(os.path.basename(i))[0] for i in text_files]\n",
    "    hex_file_base_list = [os.path.splitext(os.path.basename(i))[0] for i in hex_files]\n",
    "\n",
    "    # files in text list not in hex list\n",
    "    no_match_text_files = set(text_file_base_list) - set(hex_file_base_list)\n",
    "\n",
    "    # files in hex list not in text list\n",
    "    no_match_hex_files = set(hex_file_base_list) - set(text_file_base_list)\n",
    "\n",
    "    if len(no_match_text_files) > 0:\n",
    "        print(f\"There are not hex file matches for the following text files: {no_match_text_files}\")\n",
    "\n",
    "    if len(no_match_hex_files) > 0:\n",
    "        print(f\"There are not text file matches for the following hex files: {no_match_hex_files}\")\n",
    "\n",
    "    # hex file size should be smaller than the text file or something may be wrong\n",
    "    text_file_sizes = [os.stat(i).st_size for i in text_files]\n",
    "    hex_file_sizes = [os.stat(i).st_size for i in hex_files]\n",
    "\n",
    "    for index, i in enumerate(text_files):\n",
    "        \n",
    "        text_file_size = os.stat(i).st_size\n",
    "        hex_file_size = os.stat(f\"{os.path.splitext(i)[0]}.hex\").st_size\n",
    "\n",
    "        if hex_file_size >= text_file_size:\n",
    "            print(f\"WARNING:  Text file '{i}' is smaller than or equal to the hex file size (bytes).\")\n",
    "            print(f\"Text file size: {text_file_size}, Hex file size: {hex_file_size}, Difference (hex - text): {hex_file_size - text_file_size}\")\n",
    "            print(f\"Removing files from import.  Please review.\")\n",
    "\n",
    "            # remove files with incorrect sizes\n",
    "            text_files.remove(text_files[index])\n",
    "            \n",
    "    return text_files\n",
    "\n",
    "\n",
    "def whitespace_to_csv(input_file:str, \n",
    "                      output_dir:str) -> str:\n",
    "    \"\"\"Generate new output files with whitespace converted to CSV.\"\"\"\n",
    "    \n",
    "    # extract basename from input file\n",
    "    basename = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    \n",
    "    # construct output file name\n",
    "    output_file = os.path.join(output_dir, f\"{basename}.csv\")\n",
    "    \n",
    "    # open file to write\n",
    "    with open(output_file, \"w\") as out:\n",
    "        \n",
    "        # read input file as string\n",
    "        with open(input_file) as get:\n",
    "            content = get.read()\n",
    "            \n",
    "            # write content replacing any whitespace with commas but keeping new lines or carriage returns\n",
    "            out.write(re.sub(\"[^\\S^\\r\\n]+\", \",\", content))\n",
    "            \n",
    "    return output_file\n",
    "\n",
    "\n",
    "def orion_raw_to_parquet(input_file: str,\n",
    "                         output_directory: str,\n",
    "                         target_frequency_list: list,\n",
    "                         target_code_list: list) -> str:\n",
    "    \"\"\"Create a parquet file for each formatted CSV.\"\"\"\n",
    "    \n",
    "    # extract file name from input file\n",
    "    file_name = f\"0_ORION_{os.path.splitext(os.path.basename(input_file))[0]}\"\n",
    "    \n",
    "    # create output file name\n",
    "    output_parquet_file = os.path.join(output_directory, f\"{file_name}.parquet\")\n",
    "\n",
    "\n",
    "    sql = f\"\"\"\n",
    "    COPY(\n",
    "        SELECT\n",
    "            concat(\n",
    "                CASE\n",
    "                    WHEN length(Freq::VARCHAR) = 4\n",
    "                    THEN Freq::VARCHAR || '000'\n",
    "                    WHEN length(Freq::VARCHAR) = 5\n",
    "                    THEN Freq::VARCHAR || '00'        \n",
    "                    WHEN length(Freq::VARCHAR) = 6\n",
    "                    THEN Freq::VARCHAR || '0' \n",
    "                    ELSE Freq::VARCHAR \n",
    "                END\n",
    "                ,'.'\n",
    "                ,CASE\n",
    "                    WHEN length(Code::VARCHAR) = 1\n",
    "                    THEN '00' || Code::VARCHAR\n",
    "                    WHEN length(Code::VARCHAR) = 2\n",
    "                    THEN '0' || Code::VARCHAR  \n",
    "                    ELSE Code::VARCHAR\n",
    "                END\n",
    "            ) AS fish_id\n",
    "            ,(Date + Time) AS date_time\n",
    "            ,Site AS receiver_id\n",
    "            ,Power AS signal_power\n",
    "            ,'{file_name}' AS file_nm\n",
    "        FROM\n",
    "            read_csv_auto('{input_file}')\n",
    "        WHERE\n",
    "            Freq IS NOT NULL\n",
    "            AND Code IS NOT NULL\n",
    "            AND Date IS NOT NULL\n",
    "            AND Time IS NOT NULL\n",
    "            AND Site IS NOT NULL\n",
    "            AND Power IS NOT NULL\n",
    "            AND Type IS NOT NULL \n",
    "            AND Type = 'LOTEK'\n",
    "            AND Freq IN {tuple(target_frequency_list)}\n",
    "            AND Code IN {tuple(target_code_list)}\n",
    "        ) TO '{output_parquet_file}' (FORMAT PARQUET);\n",
    "    \"\"\"\n",
    "    \n",
    "    # execute query\n",
    "    duckdb.query(sql)\n",
    "    \n",
    "    return output_parquet_file\n",
    "\n",
    "\n",
    "def generate_orion_parquet_files(orion_dir: str,\n",
    "                                 target_frequency_list: List[float],\n",
    "                                 target_code_list: List[int],\n",
    "                                 output_directory: str) -> List[str]:\n",
    "    \"\"\"Generate ORION parquet files for query.\"\"\"\n",
    "    \n",
    "    # generate the full file list to process\n",
    "    file_list = tqdm(generate_orion_import_file_list(orion_dir))\n",
    "\n",
    "    # process files\n",
    "    processed_files = []\n",
    "    for i in file_list:\n",
    "\n",
    "        # convert all whitespace in Orion text files to commas\n",
    "        raw_csv_file = whitespace_to_csv(i, orion_dir)\n",
    "\n",
    "        # convert CSV file to parquet format\n",
    "        parquet_file = orion_raw_to_parquet(input_file=raw_csv_file,\n",
    "                                            output_directory=output_directory,\n",
    "                                            target_frequency_list=target_frequency_list,\n",
    "                                            target_code_list=target_code_list)\n",
    "        # add processed file to output list\n",
    "        processed_files.append(parquet_file)\n",
    "    \n",
    "    return processed_files\n",
    "\n",
    "\n",
    "def mitas_raw_to_parquet(input_file: str,\n",
    "                         output_directory: str,\n",
    "                         target_frequency_list: List[float],\n",
    "                         target_code_list: List[int]):\n",
    "    \"\"\"Ingest and format an input MITAS file.\"\"\"\n",
    "        \n",
    "    # extract file name from input file\n",
    "    file_name = f\"1_MITAS_{os.path.splitext(os.path.basename(input_file))[0]}\"\n",
    "    \n",
    "    # create output file name\n",
    "    output_parquet_file = os.path.join(output_directory, f\"{file_name}.parquet\")\n",
    "    \n",
    "    # Erin added datetime conversion of UTC-04:00 datetime to UTC-07:00 datetime\n",
    "    sql = f\"\"\"\n",
    "    COPY(\n",
    "        SELECT\n",
    "            concat(\n",
    "                CASE\n",
    "                    WHEN length(frequency::VARCHAR) = 4\n",
    "                    THEN frequency::VARCHAR || '000'\n",
    "                    WHEN length(frequency::VARCHAR) = 5\n",
    "                    THEN frequency::VARCHAR || '00'        \n",
    "                    WHEN length(frequency::VARCHAR) = 6\n",
    "                    THEN frequency::VARCHAR || '0' \n",
    "                    ELSE frequency::VARCHAR \n",
    "                END\n",
    "                ,'.'\n",
    "                ,CASE\n",
    "                    WHEN length(codeNumber::VARCHAR) = 1\n",
    "                    THEN '00' || codeNumber::VARCHAR\n",
    "                    WHEN length(codeNumber::VARCHAR) = 2\n",
    "                    THEN '0' || codeNumber::VARCHAR  \n",
    "                    ELSE codeNumber::VARCHAR\n",
    "                END\n",
    "            ) AS fish_id\n",
    "            ,\"decodeTimeUTC-04:00\" - interval '3 hours' AS date_time\n",
    "            ,ReceiverId AS receiver_id\n",
    "            ,power::INT AS signal_power\n",
    "            ,'{file_name}' AS file_nm\n",
    "        FROM\n",
    "            '{input_file}'\n",
    "        WHERE\n",
    "            frequency IS NOT NULL\n",
    "            AND codeNumber IS NOT NULL\n",
    "            AND \"decodeTimeUTC-04:00\" IS NOT NULL\n",
    "            AND ReceiverId IS NOT NULL\n",
    "            AND power IS NOT NULL\n",
    "            AND frequency IN {tuple(target_frequency_list)}\n",
    "            AND codeNumber IN {tuple(target_code_list)}\n",
    "        ) TO '{output_parquet_file}' (FORMAT PARQUET);\n",
    "    \"\"\"\n",
    "    \n",
    "    # fire query\n",
    "    duckdb.query(sql)\n",
    "    \n",
    "    return output_parquet_file\n",
    "\n",
    "\n",
    "def generate_mitas_parquet_files(mitas_dir: str,\n",
    "                                 target_frequency_list: List[float],\n",
    "                                 target_code_list: List[int],\n",
    "                                 output_directory: str) -> List[str]:\n",
    "    \"\"\"Generate MITAS parquet files for use in query.\"\"\"\n",
    "    \n",
    "    # get a list of mitas CSV files\n",
    "    mitas_csv_files = glob.glob(os.path.join(mitas_dir, \"*.csv\"))\n",
    "\n",
    "    processed_files = []\n",
    "    for i in tqdm(mitas_csv_files):\n",
    "\n",
    "        # convert each file to a parquet file\n",
    "        output_file = mitas_raw_to_parquet(input_file=i, \n",
    "                                           output_directory=output_directory,\n",
    "                                           target_frequency_list=target_frequency_list,\n",
    "                                           target_code_list=target_code_list)\n",
    "        \n",
    "        # add processed file to output list\n",
    "        processed_files.append(output_file)\n",
    "        \n",
    "    return processed_files\n",
    "\n",
    "\n",
    "def filter_tagged_fish(df, tagging_df):\n",
    "    \"\"\"Only keep fish in tagging file.\"\"\"\n",
    "    \n",
    "    n_records = df.shape[0]\n",
    "\n",
    "    # only keep fish in tagging file\n",
    "    df = df.loc[df[\"fish_id\"].isin(tagging_df[\"fish_id\"].unique())]\n",
    "\n",
    "    n_dropped = n_records - df.shape[0]\n",
    "\n",
    "    print(f\"Dropped {n_dropped} records for fish not in tagging file.\")\n",
    "    \n",
    "    return df \n",
    "\n",
    "\n",
    "def filter_release_time(df, tagging_df):\n",
    "    \"\"\"Only keep records greater than or equal to release time.\"\"\"\n",
    "    \n",
    "    n_records = df.shape[0]\n",
    "\n",
    "    # get a lookup dictionary of release date times from each fish\n",
    "    fish_release_time_dict = tagging_df.set_index(\"fish_id\")[\"tag_release_date_pst\"].to_dict()\n",
    "\n",
    "    # add field for release time to bound study start\n",
    "    df[\"tag_release_date_pst\"] = df[\"fish_id\"].map(fish_release_time_dict)\n",
    "\n",
    "    # only keep records greater than the fish release time\n",
    "    df = df.loc[df[\"date_time\"] >= df[\"tag_release_date_pst\"]].copy()\n",
    "\n",
    "    df.drop(columns=[\"tag_release_date_pst\"], inplace=True)\n",
    "\n",
    "    n_dropped = n_records - df.shape[0]\n",
    "\n",
    "    print(f\"Dropped {n_dropped} records for detections before release time.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_tag_life(df, end_date_time=\"2022-11-01 00:00:00\"):\n",
    "    \"\"\"Only keep records that span through tag life.\"\"\"\n",
    "    \n",
    "    n_records = df.shape[0]\n",
    "\n",
    "    # only keep records that account for tag life\n",
    "    df = df.loc[df[\"date_time\"] <= end_date_time]\n",
    "\n",
    "    n_dropped = n_records - df.shape[0]\n",
    "\n",
    "    print(f\"Dropped {n_dropped} records exceeding study period date and time.\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_drop_duplicate_detections(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Drop duplicate detections by keeping the first files which are Orion files.\"\"\"\n",
    "\n",
    "    n_records = df.shape[0]\n",
    "\n",
    "    # drop duplicates by keeping \"first\" which are the orion files\n",
    "    df.drop_duplicates(subset=[\"fish_id\", \"date_time\", \"site_number\", \"signal_power\"], \n",
    "                       keep=\"first\", \n",
    "                       inplace=True)\n",
    "\n",
    "    n_dropped = n_records - df.shape[0]\n",
    "\n",
    "    print(f\"Dropped {n_dropped} duplicate MITAS and ORION records.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_raw_data(glob_path: str,\n",
    "                    reciever_to_detect_site_dict: dict,\n",
    "                    receiver_to_site_number_dict: dict,\n",
    "                    tagging_df: pd.DataFrame,\n",
    "                    project_end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Apply filters to raw data and add detection site and site number.\n",
    "    \n",
    "    Filter 1:  only expected fish in the data \n",
    "    Filter 2:  ensures the fish times are bound by release datetime\n",
    "    Filter 3:  ensures that only detections that fall into the tag life window are considered\n",
    "    Filter 4:  drop duplicates occurring in MITAS and ORION; keep ORION by default\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM \n",
    "        '{glob_path}'\n",
    "    WHERE\n",
    "        fish_id = '{i}';\n",
    "    \"\"\"\n",
    "    \n",
    "    df = duckdb.query(sql).df()\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        print(f\"WARNING:  There were no valid detections for fish_id:  '{i}'\")\n",
    "        print(f\"WARNING:  Output file will not be created.\")\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        # add detect site\n",
    "        df[\"detect_site\"] = df[\"receiver_id\"].map(reciever_to_detect_site_dict)\n",
    "\n",
    "        # change receiver id to site number\n",
    "        df[\"site_number\"] = df[\"receiver_id\"].map(receiver_to_site_number_dict)\n",
    "\n",
    "        # ensure that only expected fish are in the data\n",
    "        df = filter_tagged_fish(df, tagging_df)\n",
    "\n",
    "        # ensure that fish times are bound by release time\n",
    "        df = filter_release_time(df, tagging_df)\n",
    "\n",
    "        # ensure that only detections that fall into the tag life window are considered\n",
    "        df = filter_tag_life(df,\n",
    "                             end_date_time=project_end_date)\n",
    "\n",
    "\n",
    "        # drop duplicates by keeping \"first\" which are the orion files\n",
    "        df = filter_drop_duplicate_detections(df)\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_lag_lead_records(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create lag and lead records for the target fish to show what happened before\n",
    "    and after the target.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # sort data frame\n",
    "    df.sort_values(by=[\"fish_id\", \"detect_site\", \"date_time\"], inplace=True)\n",
    "\n",
    "    # reindex dataset\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # lag\n",
    "    lag_df = df.shift(periods=1)[[\"fish_id\", \"detect_site\", \"date_time\"]]\n",
    "\n",
    "    # lead\n",
    "    lead_df = df.shift(periods=-1)[[\"fish_id\", \"detect_site\", \"date_time\"]]\n",
    "\n",
    "    # add to main data frame\n",
    "    df[\"lag_fish_id\"] = lag_df[\"fish_id\"]\n",
    "    df[\"lag_detect_site\"] = lag_df[\"detect_site\"]\n",
    "    df[\"lag_date_time\"] = lag_df[\"date_time\"]\n",
    "    df[\"lead_fish_id\"] = lead_df[\"fish_id\"]\n",
    "    df[\"lead_detect_site\"] = lead_df[\"detect_site\"]\n",
    "    df[\"lead_date_time\"] = lead_df[\"date_time\"]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_events(df, hits, seconds, detection_sites):\n",
    "    \n",
    "    df = df.loc[df[\"detect_site\"].isin(detection_sites)].copy()\n",
    "\n",
    "    # create timedelta field in seconds; set\n",
    "    df['time_from_previous_hit'] = np.where(\n",
    "                                    (df.fish_id == df.lag_fish_id) & (df.detect_site == df.lag_detect_site),\n",
    "                                    (df.date_time - df.lag_date_time).fillna(pd.Timedelta('0 days')).values.view('<i8')/10**9,\n",
    "                                    -1)\n",
    "\n",
    "    # create block_id for each event where hits are no more than time threshold seconds apart\n",
    "    df['block_id'] = ((df.time_from_previous_hit >= seconds) | (df.time_from_previous_hit < 0)).astype(int).cumsum()\n",
    "\n",
    "    # get hit count of each block\n",
    "    df['block_count'] = df.groupby(['block_id'])['block_id'].transform('count')\n",
    "\n",
    "    # remove unneeded columns\n",
    "    drop_cols = ['time_from_previous_hit', 'lead_fish_id', 'lag_fish_id', 'lead_detect_site', 'lag_detect_site',\n",
    "                 'lead_date_time', 'lag_date_time']\n",
    "\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    # only keep event blocks that meet the hits per block threshold\n",
    "    df = df[df['block_count'] >= hits]\n",
    "    \n",
    "    # add in hits per sec claus\n",
    "    df[\"grouping\"] = f\"{hits}_{seconds}\"\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b270e99-3e26-47bf-8e4d-8fb3445d9802",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710bdebc-744b-42ef-90f4-75ea08fd122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project directory\n",
    "# root_dir = \"/Users/d3y010/projects/telemetry/mm_sum_2022\"\n",
    "root_dir = \"C:/Users/mcca512/OneDrive - PNNL/McCann Documents/MMD Adult Study/mmd_sum_2022\"\n",
    "\n",
    "# data directory holding raw data\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "# mitas raw data directory\n",
    "mitas_dir = os.path.join(data_dir, \"mitas\")\n",
    "\n",
    "# orion raw data directory\n",
    "orion_dir = os.path.join(data_dir, \"orion\")\n",
    "\n",
    "# directory to hold formatted raw parquet files for query\n",
    "parquet_raw_dir = os.path.join(data_dir, \"parquet_raw_data\")\n",
    "\n",
    "# output directory\n",
    "output_dir = os.path.join(root_dir, \"outputs\")\n",
    "\n",
    "# project start date\n",
    "project_start_date = \"2022-06-02 00:00:00\"\n",
    "\n",
    "# project end date\n",
    "project_end_date = \"2022-11-01 00:00:00\"\n",
    "\n",
    "# supporting files\n",
    "beacon_file = os.path.join(root_dir, \"data\", \"load_db\", \"tbl_beacons_mmd_summer_20221011.csv\")\n",
    "tagging_file = os.path.join(root_dir, \"data\", \"load_db\", \"acttagrel_mmd_rt_20221102.xlsx\")\n",
    "dam_ops_file = os.path.join(root_dir, \"data\", \"load_db\", \"Hourly_dam_ops_foster_2022_final_091522.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f890d-232a-4564-90f7-15dbe62d51fc",
   "metadata": {},
   "source": [
    "## Read in beacon and tagging files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a3220d-7d14-4d25-a874-7d5fe8c7580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in beacon file\n",
    "beacon_df = pd.read_csv(beacon_file)\n",
    "\n",
    "# # build a dictionary of receiver id to detect site\n",
    "reciever_to_detect_site_dict = beacon_df.set_index(\"receiver_id\")[\"detect_site\"].to_dict()\n",
    "\n",
    "# construct a dictionary of receiver id to site number\n",
    "receiver_to_site_number_dict = beacon_df.set_index(\"receiver_id\")[\"site_number\"].to_dict()\n",
    "\n",
    "# read in tagging data\n",
    "tagging_df = read_tagging_file(tagging_file)\n",
    "\n",
    "# create a list of valid fish ids to process\n",
    "fish_array = tagging_df[\"fish_id\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09ba47f-5e58-429d-814e-6bfc1a8299ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate lists of expected frequencies and codes from tagging file\n",
    "fish_id_array = tagging_df[\"fish_id\"].values\n",
    "target_frequency_list = np.unique([float(i[:7]) for i in fish_id_array])\n",
    "target_code_list = np.unique([int(i[-3:]) for i in fish_id_array])\n",
    "\n",
    "# generate a list of expected site numbers from the beacons file\n",
    "target_site_number_list = beacon_df[\"site_number\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4ff16-913b-4faa-b738-a7d17180c2a7",
   "metadata": {},
   "source": [
    "## Convert native input files to parquet files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad94f6-c4ef-4d3b-b060-799977f79c6e",
   "metadata": {},
   "source": [
    "Once the MITAS and ORION files have been built, they do not need to be built again unless new data is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4449a4cc-0561-42fe-b546-654fce4f4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate formatted raw files into parquet format for query\n",
    "mitas_raw_parquet_files = generate_mitas_parquet_files(mitas_dir=mitas_dir,\n",
    "                                                       target_frequency_list=target_frequency_list,\n",
    "                                                       target_code_list=target_code_list, \n",
    "                                                       output_directory=parquet_raw_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d7a11-37a0-40b4-861d-3f1cd8108c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate formatted raw files into parquet format for query\n",
    "orion_raw_parquet_files = generate_orion_parquet_files(orion_dir=orion_dir,\n",
    "                                                       target_frequency_list=target_frequency_list,\n",
    "                                                       target_code_list=target_code_list, \n",
    "                                                       output_directory=parquet_raw_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bb4835-dfd4-4cd9-8819-0ac2c39fa9a2",
   "metadata": {},
   "source": [
    "# Start here with new fish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a8bc69-e240-4d3e-947b-8248ca323461",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fish_id = \"166.620.103\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108dba06-4076-4832-bbb1-7473f29f71e6",
   "metadata": {},
   "source": [
    "## Filter raw data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698458a9-8806-4429-b3c7-82aeb3f5adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# get path to glob all raw parquet files\n",
    "glob_path = os.path.join(parquet_raw_dir, \"*.parquet\")\n",
    "\n",
    "# fish_iterator = tqdm([fish_array[3]])\n",
    "\n",
    "fish_iterator = [test_fish_id]\n",
    "\n",
    "for i in fish_iterator:\n",
    "    \n",
    "    df = filter_raw_data(glob_path,\n",
    "                         reciever_to_detect_site_dict=reciever_to_detect_site_dict,\n",
    "                         receiver_to_site_number_dict=receiver_to_site_number_dict,\n",
    "                         tagging_df=tagging_df,\n",
    "                         project_end_date=project_end_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0fadb8-f057-4911-a8b1-e120d66b7b82",
   "metadata": {},
   "source": [
    "## Create lag/lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1d407-c811-40ef-8fdb-198f633b9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = generate_lag_lead_records(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7e4a9-21d9-484f-a318-7b85ec00ac0f",
   "metadata": {},
   "source": [
    "## Create events with no dam operations considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbfb057-62dc-4246-98fd-c0e45d7eb420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of detection sites per hits per second condition\n",
    "cond_one = beacon_df.loc[beacon_df[\"hits_seconds_run\"] == \"3_60\"][\"detect_site\"].to_list()\n",
    "cond_two = beacon_df.loc[beacon_df[\"hits_seconds_run\"] == \"2_120\"][\"detect_site\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee3626-0f2a-4f9f-86d2-48d52b53475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# create events per condition\n",
    "dfa = create_events(df, hits=3, seconds=60, detection_sites=cond_one)\n",
    "dfb = create_events(df, hits=2, seconds=120, detection_sites=cond_two)\n",
    "\n",
    "# merge output\n",
    "events_nops = pd.concat([dfa, dfb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939b49b-5c80-4582-b96a-64a5c6fc8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 95th percentile of signal power to filter events where 95% of all records fall under\n",
    "minimum_signal_power = events_nops[\"signal_power\"].quantile(q=0.95)\n",
    "\n",
    "print(f\"Signal power threshold for 95th percentile:  {minimum_signal_power}\")\n",
    "\n",
    "# filter events by threshold\n",
    "events_nops = events_nops.loc[events_nops[\"signal_power\"] >= minimum_signal_power]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6047d7cc-3e07-4202-bdc3-d344084c3ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = events_nops.copy()\n",
    "\n",
    "# df[\"date\"] = df[\"date_time\"].dt.date\n",
    "\n",
    "df[\"date\"] = df[\"date_time\"].dt.round(\"S\").dt.strftime('%m-%d %H:00:00')\n",
    "\n",
    "df = df.loc[df[\"signal_power\"] >= minimum_signal_power]\n",
    "\n",
    "plot_detections = df.groupby([\"detect_site\", \"date\"]).count().reset_index().pivot(\"detect_site\", \"date\", \"fish_id\")\n",
    "\n",
    "# more negative power is weaker; closer to 0 is more powerful\n",
    "plot_power = df.groupby([\"detect_site\", \"date\"]).max().reset_index().pivot(\"detect_site\", \"date\", \"signal_power\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33a579-ec44-4591-834c-23e54972d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(25, 4))\n",
    "\n",
    "# g = sns.heatmap(plot_detections, annot=False, linewidths=.5, ax=ax)\n",
    "g = sns.heatmap(plot_detections, annot=False, ax=ax, cmap=\"vlag\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af864e8-abb1-4059-a80c-66c9439b8457",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(25, 3))\n",
    "\n",
    "g = sns.heatmap(plot_power, annot=False, cmap=\"vlag\", ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd074d36-6feb-446c-8ec4-3650ae897d2e",
   "metadata": {},
   "source": [
    "## format events for use in travel time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d926a4f1-c40f-4109-b2b6-c622eedc5227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 13 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14, 5)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# dictionary of detetion sites to travel time file abbreviation\n",
    "detect_site_to_abbrev_dict = {1: \"sw\",\n",
    "                              2: \"e1\",\n",
    "                              3: \"e2\",\n",
    "                              4: \"l1\",\n",
    "                              5: \"l2\",\n",
    "                              6: \"l3\",\n",
    "                              7: \"ps\"}\n",
    "\n",
    "\n",
    "# generate last event time for each detection site\n",
    "events_nops_last = events_nops.groupby([\"fish_id\", \"grouping\", \"detect_site\"])[\"date_time\"].max().reset_index()\n",
    "events_nops_last[\"site_abbrev\"] = events_nops_last[\"detect_site\"].map(detect_site_to_abbrev_dict).str.lower() + \"_l\"\n",
    "\n",
    "# generate first event time for each detection site\n",
    "events_nops_first = events_nops.groupby([\"fish_id\", \"grouping\", \"detect_site\"])[\"date_time\"].min().reset_index()\n",
    "events_nops_first[\"site_abbrev\"] = events_nops_first[\"detect_site\"].map(detect_site_to_abbrev_dict).str.lower() + \"_f\"\n",
    "\n",
    "# combine\n",
    "events_nops_tt = pd.concat([events_nops_first, events_nops_last])\n",
    "\n",
    "events_nops_tt.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "937ebb07-67b0-4c59-bec5-486cb83483a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fish_id</th>\n",
       "      <th>grouping</th>\n",
       "      <th>detect_site</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_abbrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>3_60</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-08 05:58:47</td>\n",
       "      <td>sw_f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>3_60</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-11 18:44:15</td>\n",
       "      <td>sw_l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-09 17:14:17</td>\n",
       "      <td>e1_f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-11 19:11:34</td>\n",
       "      <td>e1_l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-09 17:18:02</td>\n",
       "      <td>e2_f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-11 19:16:03</td>\n",
       "      <td>e2_l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-09 17:24:22</td>\n",
       "      <td>l1_f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-11 19:17:37</td>\n",
       "      <td>l1_l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-07-09 17:32:32</td>\n",
       "      <td>l2_f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-07-11 19:24:01</td>\n",
       "      <td>l2_l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-07-09 17:32:48</td>\n",
       "      <td>l3_f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>6</td>\n",
       "      <td>2022-07-11 19:26:19</td>\n",
       "      <td>l3_l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>7</td>\n",
       "      <td>2022-07-11 19:37:36</td>\n",
       "      <td>ps_f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>166.620.103</td>\n",
       "      <td>2_120</td>\n",
       "      <td>7</td>\n",
       "      <td>2022-07-12 09:02:02</td>\n",
       "      <td>ps_l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fish_id grouping  detect_site           date_time site_abbrev\n",
       "6  166.620.103     3_60            1 2022-07-08 05:58:47        sw_f\n",
       "6  166.620.103     3_60            1 2022-07-11 18:44:15        sw_l\n",
       "0  166.620.103    2_120            2 2022-07-09 17:14:17        e1_f\n",
       "0  166.620.103    2_120            2 2022-07-11 19:11:34        e1_l\n",
       "1  166.620.103    2_120            3 2022-07-09 17:18:02        e2_f\n",
       "1  166.620.103    2_120            3 2022-07-11 19:16:03        e2_l\n",
       "2  166.620.103    2_120            4 2022-07-09 17:24:22        l1_f\n",
       "2  166.620.103    2_120            4 2022-07-11 19:17:37        l1_l\n",
       "3  166.620.103    2_120            5 2022-07-09 17:32:32        l2_f\n",
       "3  166.620.103    2_120            5 2022-07-11 19:24:01        l2_l\n",
       "4  166.620.103    2_120            6 2022-07-09 17:32:48        l3_f\n",
       "4  166.620.103    2_120            6 2022-07-11 19:26:19        l3_l\n",
       "5  166.620.103    2_120            7 2022-07-11 19:37:36        ps_f\n",
       "5  166.620.103    2_120            7 2022-07-12 09:02:02        ps_l"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_nops_tt.sort_values(by=\"detect_site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c89f9-1f92-4213-905c-ab64f4e2f39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "023fe617-0bf0-4cf6-89f1-2f6dbe7c2f80",
   "metadata": {},
   "source": [
    "# The following is in-development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a2a8b-0121-40c8-9d55-ba66df18421d",
   "metadata": {},
   "source": [
    "## generate travel time with no ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "95e3682e-073b-4f70-8616-9015a14a8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of data types for the travel time data frame\n",
    "travel_time_template_dtypes = {\n",
    "    # \"fish_id\": str,\n",
    "    \"pit_code\": str,\n",
    "    \"rel_name\": str,\n",
    "    \"lot\": int,\n",
    "    \"srr\": str,\n",
    "    \"act_datetime\": \"datetime64[ns]\",\n",
    "    \"release_datetime\": \"datetime64[ns]\",\n",
    "    \"hole\": str,\n",
    "    \"subhole\": str,\n",
    "    \"sw\": int,\n",
    "    \"e1\": int,\n",
    "    \"e2\": int,\n",
    "    \"l1\": int,\n",
    "    \"l2\": int,\n",
    "    \"l3\": int,\n",
    "    \"ps\": int,\n",
    "    \"sw_f\": \"datetime64[ns]\",\n",
    "    \"sw_l\": \"datetime64[ns]\",\n",
    "    \"e1_f\": \"datetime64[ns]\",\n",
    "    \"e1_l\": \"datetime64[ns]\",\n",
    "    \"e2_f\": \"datetime64[ns]\",\n",
    "    \"e2_l\": \"datetime64[ns]\",\n",
    "    \"l1_f\": \"datetime64[ns]\",\n",
    "    \"l1_l\": \"datetime64[ns]\",\n",
    "    \"l2_f\": \"datetime64[ns]\",\n",
    "    \"l2_l\": \"datetime64[ns]\",\n",
    "    \"l3_f\": \"datetime64[ns]\",\n",
    "    \"l3_l\": \"datetime64[ns]\",\n",
    "    \"ps_f\": \"datetime64[ns]\",\n",
    "    \"ps_l\": \"datetime64[ns]\",\n",
    "    \"rel_weir_time_s\": int,\n",
    "    \"pool_stage\": str,\n",
    "    \"censor\": int,\n",
    "    \"altered\": int,\n",
    "    \"mort_xlat\": int,\n",
    "    \"release_stage\": str,\n",
    "    \"comments\": str\n",
    "}\n",
    "\n",
    "# build a dictionary of field with empty data\n",
    "travel_time_template_dict = {i: [] for i in travel_time_template_dtypes.keys()}\n",
    "\n",
    "# generate travel time template\n",
    "travel_time_template = pd.DataFrame(travel_time_template_dict).astype(travel_time_template_dtypes)\n",
    "\n",
    "# add fish ids to the travel time file\n",
    "travel_time_template[\"fish_id\"] = tagging_df.index\n",
    "\n",
    "# set template site designation to 0 \n",
    "detection_site_abbrev_list = [\"sw\", \"e1\", \"e2\", \"l1\", \"l2\", \"l3\", \"ps\"]\n",
    "travel_time_template[detection_site_abbrev_list] = 0\n",
    "\n",
    "# generate a list of detection site first and last columns in order\n",
    "detection_site_time_columns = []\n",
    "for i in detection_site_abbrev_list:\n",
    "    detection_site_time_columns.append(f\"{i}_f\")\n",
    "    detection_site_time_columns.append(f\"{i}_l\")\n",
    "\n",
    "# sort by fish id\n",
    "travel_time_template.sort_values(by=[\"fish_id\"], inplace=True)\n",
    "\n",
    "# set index to fish id\n",
    "travel_time_template.set_index(\"fish_id\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "654a6647-03ba-4693-a726-8979dafb7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_nops_tt_layout = pd.pivot_table(events_nops_tt,\n",
    "                                       values=\"date_time\",\n",
    "                                       index=[\"fish_id\"],\n",
    "                                       columns=[\"site_abbrev\"]).rename_axis(None, axis=1)\n",
    "\n",
    "# add empty columns to make full travel time input\n",
    "events_nops_tt_layout_columns = events_nops_tt_layout.columns\n",
    "\n",
    "for i in travel_time_template_dtypes.keys():\n",
    "    \n",
    "    if i not in events_nops_tt_layout_columns:\n",
    "        events_nops_tt_layout[i] = None\n",
    "\n",
    "# reorder columns\n",
    "events_nops_tt_layout = events_nops_tt_layout[travel_time_template_dtypes.keys()]\n",
    "\n",
    "events_nops_tt_layout.to_csv(\"/Users/d3y010/Desktop/template_no-ops.csv\", index=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cda8d481-a1e5-41e5-8cc2-4c1d0947efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_ops = events_nops.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f3041f0-962a-4d2d-880c-2782de4d3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update template fields for each detection site where an event was logged regardless of whether or not the fish passed the site\n",
    "x = events_ops.set_index(\"fish_id\")[\"detect_site\"].map({i: detect_site_to_abbrev_dict[i].lower() for i in detect_site_to_abbrev_dict.keys()}).reset_index()\n",
    "\n",
    "x[\"detected\"] = 1\n",
    "\n",
    "detect_designation = pd.pivot_table(x, values=\"detected\", index=\"fish_id\", columns=\"detect_site\", fill_value=0).rename_axis(None, axis=1)\n",
    "\n",
    "# update the template data frame with the new data\n",
    "travel_time_template.update(detect_designation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ccfde186-74bb-4100-a191-a480cefc1259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build release name needed in travel time file\n",
    "tagging_df[\"rel_name\"] = tagging_df[\"release_location_xlat\"] + \"_\" + tagging_df[\"srr\"]\n",
    "\n",
    "# extract that tagging file information needed for the travel time data frame\n",
    "tagging_for_tt = tagging_df[[\"pit_code\", \"rel_name\", \"srr\", \"tag_activation_date_pst\", \"tag_release_date_pst\", \"mort_xlat\"]].copy()\n",
    "\n",
    "# rename columns to what is expected in the travel time data frame\n",
    "tagging_for_tt.rename(columns={\"tag_activation_date_pst\": \"act_datetime\", \n",
    "                               \"tag_release_date_pst\": \"release_datetime\"},\n",
    "                      inplace=True)\n",
    "\n",
    "# update the template data frame with the new data\n",
    "travel_time_template.update(tagging_for_tt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c23684b-feb3-49d2-a306-7e1e0b054456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pit_code</th>\n",
       "      <th>rel_name</th>\n",
       "      <th>lot</th>\n",
       "      <th>srr</th>\n",
       "      <th>act_datetime</th>\n",
       "      <th>release_datetime</th>\n",
       "      <th>hole</th>\n",
       "      <th>subhole</th>\n",
       "      <th>sw</th>\n",
       "      <th>e1</th>\n",
       "      <th>...</th>\n",
       "      <th>l3_l</th>\n",
       "      <th>ps_f</th>\n",
       "      <th>ps_l</th>\n",
       "      <th>rel_weir_time_s</th>\n",
       "      <th>pool_stage</th>\n",
       "      <th>censor</th>\n",
       "      <th>altered</th>\n",
       "      <th>mort_xlat</th>\n",
       "      <th>release_stage</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fish_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3DD.003BD5543A</td>\n",
       "      <td>R1_stuck_11W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11W</td>\n",
       "      <td>2022-06-21 16:16:01</td>\n",
       "      <td>2022-06-22 12:32:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3DD.003BD55446</td>\n",
       "      <td>R1_stuck_11W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11W</td>\n",
       "      <td>2022-06-21 16:13:55</td>\n",
       "      <td>2022-06-22 12:32:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3DD.003BD55460</td>\n",
       "      <td>R1_stuck_11W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11W</td>\n",
       "      <td>2022-06-21 16:15:49</td>\n",
       "      <td>2022-06-22 12:32:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3DD.003BD5544C</td>\n",
       "      <td>R2_fpf_11W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11W</td>\n",
       "      <td>2022-06-21 16:28:37</td>\n",
       "      <td>2022-06-26 08:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3DD.003BD55440</td>\n",
       "      <td>R1_stuck_11W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11W</td>\n",
       "      <td>2022-06-21 16:26:59</td>\n",
       "      <td>2022-06-24 11:02:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>3DD.003BD55461</td>\n",
       "      <td>R3_stewart_11W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11W</td>\n",
       "      <td>2022-06-24 15:44:37</td>\n",
       "      <td>2022-06-27 14:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>3DD.003BD5541D</td>\n",
       "      <td>R3_stewart_11W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11W</td>\n",
       "      <td>2022-06-24 15:46:29</td>\n",
       "      <td>2022-06-27 14:20:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3DD.003BD5543F</td>\n",
       "      <td>R3_stewart_11W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11W</td>\n",
       "      <td>2022-06-27 19:06:38</td>\n",
       "      <td>2022-06-28 11:52:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>3DD.003BD5544F</td>\n",
       "      <td>R3_stewart_11W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11W</td>\n",
       "      <td>2022-06-27 19:05:54</td>\n",
       "      <td>2022-06-28 11:52:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>3DD.003BD55463</td>\n",
       "      <td>R3_stewart_11W</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11W</td>\n",
       "      <td>2022-06-27 19:11:18</td>\n",
       "      <td>2022-06-28 11:52:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               pit_code        rel_name  lot  srr        act_datetime  \\\n",
       "fish_id                                                                 \n",
       "0        3DD.003BD5543A    R1_stuck_11W  NaN  11W 2022-06-21 16:16:01   \n",
       "1        3DD.003BD55446    R1_stuck_11W  NaN  11W 2022-06-21 16:13:55   \n",
       "2        3DD.003BD55460    R1_stuck_11W  NaN  11W 2022-06-21 16:15:49   \n",
       "3        3DD.003BD5544C      R2_fpf_11W  NaN  11W 2022-06-21 16:28:37   \n",
       "4        3DD.003BD55440    R1_stuck_11W  NaN  11W 2022-06-21 16:26:59   \n",
       "...                 ...             ...  ...  ...                 ...   \n",
       "70       3DD.003BD55461  R3_stewart_11W  NaN  11W 2022-06-24 15:44:37   \n",
       "71       3DD.003BD5541D  R3_stewart_11W  NaN  11W 2022-06-24 15:46:29   \n",
       "72       3DD.003BD5543F  R3_stewart_11W  NaN  11W 2022-06-27 19:06:38   \n",
       "73       3DD.003BD5544F  R3_stewart_11W  NaN  11W 2022-06-27 19:05:54   \n",
       "74       3DD.003BD55463  R3_stewart_11W  NaN  11W 2022-06-27 19:11:18   \n",
       "\n",
       "           release_datetime hole subhole  sw  e1  ...  l3_l  ps_f  ps_l  \\\n",
       "fish_id                                           ...                     \n",
       "0       2022-06-22 12:32:00  NaN     NaN   0   0  ...   NaT   NaT   NaT   \n",
       "1       2022-06-22 12:32:00  NaN     NaN   0   0  ...   NaT   NaT   NaT   \n",
       "2       2022-06-22 12:32:00  NaN     NaN   0   0  ...   NaT   NaT   NaT   \n",
       "3       2022-06-26 08:30:00  NaN     NaN   0   0  ...   NaT   NaT   NaT   \n",
       "4       2022-06-24 11:02:00  NaN     NaN   0   0  ...   NaT   NaT   NaT   \n",
       "...                     ...  ...     ...  ..  ..  ...   ...   ...   ...   \n",
       "70      2022-06-27 14:20:00  NaN     NaN   0   0  ...   NaT   NaT   NaT   \n",
       "71      2022-06-27 14:20:00  NaN     NaN   0   0  ...   NaT   NaT   NaT   \n",
       "72      2022-06-28 11:52:00  NaN     NaN   0   0  ...   NaT   NaT   NaT   \n",
       "73      2022-06-28 11:52:00  NaN     NaN   0   0  ...   NaT   NaT   NaT   \n",
       "74      2022-06-28 11:52:00  NaN     NaN   0   0  ...   NaT   NaT   NaT   \n",
       "\n",
       "         rel_weir_time_s  pool_stage censor altered mort_xlat release_stage  \\\n",
       "fish_id                                                                       \n",
       "0                    NaN         NaN    NaN     NaN       0.0           NaN   \n",
       "1                    NaN         NaN    NaN     NaN       0.0           NaN   \n",
       "2                    NaN         NaN    NaN     NaN       0.0           NaN   \n",
       "3                    NaN         NaN    NaN     NaN       0.0           NaN   \n",
       "4                    NaN         NaN    NaN     NaN       0.0           NaN   \n",
       "...                  ...         ...    ...     ...       ...           ...   \n",
       "70                   NaN         NaN    NaN     NaN       0.0           NaN   \n",
       "71                   NaN         NaN    NaN     NaN       0.0           NaN   \n",
       "72                   NaN         NaN    NaN     NaN       0.0           NaN   \n",
       "73                   NaN         NaN    NaN     NaN       0.0           NaN   \n",
       "74                   NaN         NaN    NaN     NaN       0.0           NaN   \n",
       "\n",
       "        comments  \n",
       "fish_id           \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "...          ...  \n",
       "70           NaN  \n",
       "71           NaN  \n",
       "72           NaN  \n",
       "73           NaN  \n",
       "74           NaN  \n",
       "\n",
       "[75 rows x 36 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "travel_time_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1485f-85c6-4879-82cd-04c0b614c742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615c088-b6a8-4050-910b-5f400aa80937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "424bcf57-57ce-49bb-b795-6472372a5760",
   "metadata": {},
   "source": [
    "**NOTE**: apply pool and release stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403df6cf-1959-4fb4-ace2-7843701a5c67",
   "metadata": {},
   "source": [
    "### route of passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d14e3a7-0d8d-4fc9-81f1-b3c8ac39fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_dict = {4: \"SW4\",\n",
    "             5: \"SP2\",\n",
    "             6: \"SP3\",\n",
    "             7: \"SP4\",\n",
    "             9: \"PS2\",\n",
    "             10: \"PS1\"}\n",
    "\n",
    "subhole_dict = {4: \"WEIR\",\n",
    "                5: \"NON-WEIR\",\n",
    "                6: \"NON-WEIR\",\n",
    "                7: \"NON-WEIR\",\n",
    "                9: \"TURBINE\",\n",
    "                10: \"TURBINE\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9f60eab8-6409-4d16-a2e2-e1248ea1d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fish with a dam last with or without a downstream event\n",
    "dam_last_tt_fish = travel_time_template[[\"dam_l\"]].loc[~travel_time_template[[\"dam_l\"]][\"dam_l\"].isna()]\n",
    "\n",
    "route_dam_last = events_ops.loc[(events_ops[\"fish_id\"].isin(dam_last_tt_fish.index)) & \n",
    "                                (events_ops[\"date_time\"].isin(dam_last_tt_fish[\"dam_l\"])) &\n",
    "                                (events_ops[\"detect_site\"] == dam_detect_site)].sort_values(by=[\"fish_id\"]).reset_index(drop=True)[[\"fish_id\", \"site_number\"]].copy()\n",
    "\n",
    "# add hole\n",
    "route_dam_last[\"hole\"] = route_dam_last[\"site_number\"].map(hole_dict)\n",
    "\n",
    "# add subhole\n",
    "route_dam_last[\"subhole\"] = route_dam_last[\"site_number\"].map(subhole_dict)\n",
    "\n",
    "# # drop site number\n",
    "# route_dam_last.drop(columns=[\"site_number\"], inplace=True)\n",
    "\n",
    "route_dam_last.set_index(\"fish_id\", inplace=True)\n",
    "\n",
    "route_dam_hole_dict = route_dam_last[\"hole\"].to_dict()\n",
    "route_dam_subhole_dict = route_dam_last[\"subhole\"].to_dict()\n",
    "\n",
    "travel_time_template_hole = travel_time_template[\"hole\"].to_dict()\n",
    "travel_time_template_hole.update(route_dam_hole_dict)\n",
    "travel_time_template[\"hole\"] = travel_time_template.index.map(travel_time_template_hole)\n",
    "\n",
    "\n",
    "travel_time_template_subhole = travel_time_template[\"subhole\"].to_dict()\n",
    "travel_time_template_subhole.update(route_dam_subhole_dict)\n",
    "travel_time_template[\"subhole\"] = travel_time_template.index.map(travel_time_template_subhole)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbd99bba-58ef-49f5-88e3-2e23f01a561e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for fish with no dam last but a downstream event\n",
    "forebay_downstream_sites = ['dam', 'tw', 'egr', 'prm', 'leb', 'srs', 'coi', 'wil']\n",
    "\n",
    "route_fby_only = travel_time_template.loc[travel_time_template[\"fby\"] == 1][forebay_downstream_sites].copy()\n",
    "\n",
    "route_fby_only[\"hole\"] = np.where(route_fby_only[forebay_downstream_sites].sum(axis=1) == 0, \"FBY\", \"NA\")\n",
    "\n",
    "route_fby_only = route_fby_only.loc[route_fby_only[\"hole\"] == \"FBY\"]\n",
    "\n",
    "route_fby_only[\"subhole\"] = \"NaN\"\n",
    "\n",
    "route_fby_only.drop(columns=['dam', 'tw', 'egr', 'prm', 'leb', 'srs', 'coi', 'wil'], inplace=True)\n",
    "\n",
    "travel_time_template.update(route_fby_only)\n",
    "\n",
    "route_fby_only.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a95f4d8d-0800-48c7-9fc0-2b0cba5feada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# travel_time_template['gp_fby_f'] = pd.to_datetime(travel_time_template['gp_fby_f'])\n",
    "# travel_time_template['gp_fby_l'] = pd.to_datetime(travel_time_template['gp_fby_l'])\n",
    "# travel_time_template['gp_tw_f'] = pd.to_datetime(travel_time_template['gp_tw_f'])\n",
    "# travel_time_template['gp_tw_l'] = pd.to_datetime(travel_time_template['gp_tw_l'])\n",
    "# travel_time_template['sun_f'] = pd.to_datetime(travel_time_template['sun_f'])\n",
    "# travel_time_template['sun_l'] = pd.to_datetime(travel_time_template['sun_l'])\n",
    "# travel_time_template['fby_f'] = pd.to_datetime(travel_time_template['fby_f'])\n",
    "# travel_time_template['fby_l'] = pd.to_datetime(travel_time_template['fby_l'])\n",
    "# travel_time_template['bc_f'] = pd.to_datetime(travel_time_template['bc_f'])\n",
    "# travel_time_template['bc_l'] = pd.to_datetime(travel_time_template['bc_l'])\n",
    "# travel_time_template['dam_f'] = pd.to_datetime(travel_time_template['dam_f'])\n",
    "# travel_time_template['dam_l'] = pd.to_datetime(travel_time_template['dam_l'])\n",
    "# travel_time_template['tw_f'] = pd.to_datetime(travel_time_template['tw_f'])\n",
    "# travel_time_template['tw_l'] = pd.to_datetime(travel_time_template['tw_l'])\n",
    "# travel_time_template['egr_f'] = pd.to_datetime(travel_time_template['egr_f'])\n",
    "# travel_time_template['egr_l'] = pd.to_datetime(travel_time_template['egr_l'])\n",
    "# travel_time_template['prm_f'] = pd.to_datetime(travel_time_template['prm_f'])\n",
    "# travel_time_template['prm_l'] = pd.to_datetime(travel_time_template['prm_l'])\n",
    "# travel_time_template['leb_f'] = pd.to_datetime(travel_time_template['leb_f'])\n",
    "# travel_time_template['leb_l'] = pd.to_datetime(travel_time_template['leb_l'])\n",
    "# travel_time_template['srs_f'] = pd.to_datetime(travel_time_template['srs_f'])\n",
    "# travel_time_template['srs_l'] = pd.to_datetime(travel_time_template['srs_l'])\n",
    "# travel_time_template['coi_f'] = pd.to_datetime(travel_time_template['coi_f'])\n",
    "# travel_time_template['coi_l'] = pd.to_datetime(travel_time_template['coi_l'])\n",
    "# travel_time_template['wil_f'] = pd.to_datetime(travel_time_template['wil_f'])\n",
    "# travel_time_template['wil_l'] = pd.to_datetime(travel_time_template['wil_l'])\n",
    "\n",
    "\n",
    "# travel_time_template.to_csv(\"/Users/d3y010/Desktop/template_ops.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238da58-c497-4e6d-99eb-7938bbcc1d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cda20-f6a9-4bb5-919f-91b056c65979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a8c34-1a9c-45f0-b4f5-4d9eecb293d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f78b5-76f3-4936-99e3-b4fed370cc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946cea4-502f-4a00-a220-4f7e80725dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e04873-996a-46c7-b32b-792781fdcb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69086a75-5115-40e7-a025-709a12370027",
   "metadata": {},
   "source": [
    "- are we setting bc last to dam last?\n",
    "    - pupose of BC arrays (underwater antennas) was to use them to produce a route of passage if there was none (not so much to define the timestamp)\n",
    "    - bc last is always dam last when dam last exists\n",
    "    - Options:  1) remove BC arrays from travel time file, 2) or keep them;\n",
    "    \n",
    "- fby last is always dam last if exists\n",
    "- can fby first be > then fby l? or bc\"\n",
    "- for predation: do you want me to screen the FBY detections after downstream? from event validation?  [YES]\n",
    "- what does 1, 0 actually mean for sites?  is it passage or just detection?  Should these be based off of events with no ops considered?\n",
    "    - 1 means detected at every site but dam; where 1 means passed at dam and bc (should use dam considering operations)\n",
    "    - this works because there is no barrier at the other sites\n",
    "- no seconds in most of the tt revisions\n",
    "    - on me\n",
    "- is there a policy on removing detections for dam only events with no upstream or downstream events? some inconsistency here; see FBY == 0 and DAM == 1\n",
    "- explore condition for daylight savings time\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551110c-e56f-46f5-8ed8-a1526ce795ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmdrt",
   "language": "python",
   "name": "mmdrt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
